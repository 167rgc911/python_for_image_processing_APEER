{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tutorial95_DL_terminology_loss_functions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOK/2KYTp/QVLVcmvZscu43",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bnsreenu/python_for_image_processing_APEER/blob/master/tutorial95_DL_terminology_loss_functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7GtNXaCGYd-"
      },
      "source": [
        "https://youtu.be/FiXMj0Jlihk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdaBS838Wkd-"
      },
      "source": [
        "**Common loss functions explaind using simple examples**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNXfThDH5vSo",
        "outputId": "391275d7-7600-4764-8432-8005b7d0dc6b"
      },
      "source": [
        "# Mean squared error - used in regression problems\n",
        "# Average of squared distances between actual and predicted values\n",
        "# This loss value is minimized during the training process of a neural network\n",
        "\n",
        "actual = [1.2, 2.3, 3, 4.5, 5.2, 6.3, 6.9, 8.4, 9.1, 9.9]\n",
        "predicted1 = [1.1, 2.1, 3.3, 4.2, 4.9, 6., 7.2, 8., 9.5, 10.2]\n",
        "predicted2 = [0.8, 2., 2.6, 4.8, 5.7, 6.1, 6.5, 7.8, 9.5, 9.6]\n",
        "\n",
        "def mean_squared_error(actual, predicted):\n",
        "\tsum_square_error = 0.0\n",
        "\tfor i in range(len(actual)):\n",
        "\t\tsum_square_error += (actual[i] - predicted[i])**2.0\n",
        "\tmean_square_error = 1.0 / len(actual) * sum_square_error\n",
        "\treturn mean_square_error\n",
        "\n",
        "print(mean_squared_error(actual, predicted1))\n",
        "print(mean_squared_error(actual, predicted2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.09099999999999993\n",
            "0.15600000000000014\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFvaXXFc7CXa",
        "outputId": "4ef4ac6b-eab0-4d2a-e9ab-74411bda3731"
      },
      "source": [
        "# cross entropy (binary) - also referred to as log loss\n",
        "# Used for binary classification problems. Can be extended to multiclass. \n",
        "# Logarthimic penalty based on the comparison of actual and predicted classes. \n",
        "# Perfect prediction means 0 cross entropy. \n",
        "from numpy import mean\n",
        "from math import log\n",
        "\n",
        "actual_label = [0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1]\n",
        "predicted1 = [0.1, 0.9, 0.8, 0.7, 0.3, 0.2, 0.9, 0.4, 0.9, 0.99, 0.1, 0.8] \n",
        "predicted2 = [0.9, 0.1, 0.8, 0.7, 0.3, 0.2, 0.2, 0.4, 0.1, 0.99, 0.1, 0.6]  \n",
        "\n",
        "# calculate cross entropy\n",
        "def cross_entropy(p, q):     #p is actual and q is predicted value\n",
        "\treturn -sum([p[i]*log(q[i]) for i in range(len(p))])\n",
        " \n",
        "#Calculating cross entropy for each pair of actual vs predicted\n",
        "predicted_prob = predicted2  #Test predicted 1 and 2\n",
        "\n",
        "results = list()\n",
        "for i in range(len(actual)):\n",
        "  #Create list for each occurance (binary --> [Not, actual]). Can be extended to multi class.\n",
        "  actual_value = [1.0 - actual_label[i], actual_label[i]]  \n",
        "  #print(actual_value)\n",
        "  #Create list for each occurance (binary --> [Not, predicted]). Can be extended to multi class.\n",
        "  predicted_value = [1.0 - predicted_prob[i], predicted_prob[i]]\n",
        "  #print(predicted_value)\n",
        "\n",
        "  # calculate cross entropy: \n",
        "  ce = cross_entropy(actual_value, predicted_value)\n",
        "  print('actual=%.1f, predicted=%.1f --- cross_entropy: %.3f ' % (actual_label[i], predicted_prob[i], ce))\n",
        "  results.append(ce)\n",
        " \n",
        "# calculate the average cross entropy\n",
        "mean_ce = mean(results)\n",
        "print('Average Cross Entropy: ', mean_ce)\n",
        "\n",
        "# Use this guide to assess the loss during your training process\n",
        "#Cross-Entropy = 0.00: Perfect Match.\n",
        "#              < 0.02: Great match.\n",
        "#              < 0.05: Good.\n",
        "#              < 0.20: Acceptable.\n",
        "#              > 0.30: Not good.\n",
        "#              > 1.00: Bad.\n",
        "#              > 2.00 Horrible."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "actual=0.0, predicted=0.9 --- cross_entropy: 2.303 \n",
            "actual=1.0, predicted=0.1 --- cross_entropy: 2.303 \n",
            "actual=1.0, predicted=0.8 --- cross_entropy: 0.223 \n",
            "actual=1.0, predicted=0.7 --- cross_entropy: 0.357 \n",
            "actual=0.0, predicted=0.3 --- cross_entropy: 0.357 \n",
            "actual=0.0, predicted=0.2 --- cross_entropy: 0.223 \n",
            "actual=1.0, predicted=0.2 --- cross_entropy: 1.609 \n",
            "actual=0.0, predicted=0.4 --- cross_entropy: 0.511 \n",
            "actual=1.0, predicted=0.1 --- cross_entropy: 2.303 \n",
            "actual=1.0, predicted=1.0 --- cross_entropy: 0.010 \n",
            "Average Cross Entropy:  1.0197706141541614\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIyrAyzKJzJ9",
        "outputId": "341c0c09-d49d-4168-9f39-10eb5959d198"
      },
      "source": [
        "# Cross entropy using Keras\n",
        "import numpy as np\n",
        "from keras import backend\n",
        "from keras.losses import binary_crossentropy\n",
        "\n",
        "actual_label = np.array([[0, 1, 0, 1, 0], [1, 0, 1, 0, 1], [1, 1, 0, 0, 1]])\n",
        "\n",
        "#Same as actual_label\n",
        "predicted = np.array([[0, 1, 0, 1, 0], [1, 0, 1, 0, 1], [1, 1, 0, 0, 1]], dtype=np.float32)  \n",
        "\n",
        "#Class 1: Close to actual\n",
        "#Class 2: 1 misclassification (0.4 instead of 1, 2 in the middle at 0.5)\n",
        "#Class 3: 4 misclassifications\n",
        "predicted1 = np.array([[0.1, 0.8, 0.2, 0.9, 0.1], [0.4, 0.5, 0.5, 0.3, 0.7], [0.1, 0.2, 0.7, 0.6, 0.5]], dtype=np.float32) \n",
        "\n",
        "#Class 1: 3 misclassifications\n",
        "#Class 2: 0 misclassifications\n",
        "#Class 3: 5 misclassifications\n",
        "predicted2 = np.array([[0.8, 0.3, 0.8, 0.8, 0.4], [0.6, 0.2, 0.8, 0.1, 0.6], [0.1, 0.3, 0.8, 0.9, 0.1]], dtype=np.float32) \n",
        "\n",
        "# Convert the arrays to keras variables\n",
        "y_true = backend.variable(actual_label)\n",
        "y_pred = backend.variable(predicted)\n",
        "y_pred1 = backend.variable(predicted1)\n",
        "y_pred2 = backend.variable(predicted2)\n",
        "\n",
        "# calculate mean cross-entropy\n",
        "mean_ce = backend.eval(binary_crossentropy(y_true, y_pred))\n",
        "mean_ce1 = backend.eval(binary_crossentropy(y_true, y_pred1))\n",
        "mean_ce2 = backend.eval(binary_crossentropy(y_true, y_pred2))\n",
        "print('Average Cross Entropy for pred: ', mean_ce)\n",
        "print('Average Cross Entropy for pred1: ', mean_ce1)\n",
        "print('Average Cross Entropy for pred2: ', mean_ce2)\n",
        "\n",
        "#In general the loss is high for all classes even though some classes are easier to classify. \n",
        "#See loss for class 2 in pred1, we see a high value of 0.6 despite having results close to actual. \n",
        "# Focal loss addresses this issue. \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average Cross Entropy for pred:  [0. 0. 0.]\n",
            "Average Cross Entropy for pred1:  [0.1524736 0.6031868 1.3450863]\n",
            "Average Cross Entropy for pred2:  [1.0313632 0.3146596 1.9442326]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mler1f5jCKLT",
        "outputId": "163526a4-9b9b-46fa-9532-a9f55b2b2d18"
      },
      "source": [
        "#Focal loss\n",
        "from keras import backend\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "actual_label = np.array([[0, 1, 0, 1, 0], [1, 0, 1, 0, 1], [1, 1, 0, 0, 1]])\n",
        "\n",
        "#Class 1: Close to actual\n",
        "#Class 2: 1 misclassification (0.4 instead of 1, 2 in the middle at 0.5)\n",
        "#Class 3: 4 misclassifications\n",
        "predicted1 = np.array([[0.1, 0.8, 0.2, 0.9, 0.1], [0.4, 0.5, 0.5, 0.3, 0.7], [0.1, 0.2, 0.7, 0.6, 0.5]], dtype=np.float32) \n",
        "\n",
        "# Convert the arrays to keras variables\n",
        "y_true = backend.variable(actual_label)\n",
        "y_pred1 = backend.variable(predicted1)\n",
        "\n",
        "# Focal loss function\n",
        "gamma=3.  #When gamma=0, focal loss should be same as cross entropy. Try gamma=3. Higher gamma adds more weight to misclassified classes.\n",
        "alpha=0.25  #\n",
        "def focal_loss_fixed(y_true, y_pred):\n",
        "   \n",
        "\tpt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
        "\tpt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
        "\treturn -backend.mean(alpha * backend.pow(1. - pt_1, gamma) * backend.log(pt_1)) - backend.mean((1 - alpha) * backend.pow(pt_0, gamma) * backend.log(1. - pt_0))\n",
        "\n",
        "c=1  #Class\n",
        "FL_class1 = focal_loss_fixed(y_true[0], y_pred1[0])\n",
        "FL_class2 = focal_loss_fixed(y_true[1], y_pred1[1])\n",
        "FL_class3 = focal_loss_fixed(y_true[2], y_pred1[2])\n",
        "\n",
        "print('Average Focal Loss for class1: ', FL_class1)\n",
        "print('Average Focal Loss for class2: ', FL_class2)\n",
        "print('Average Focal Loss for class3: ', FL_class3)\n",
        "\n",
        "#When gamma=0, FL=CL. It is clear that the loss for second class is high eventhough it is classifying almost ok.\n",
        "#When gamma=3, The FL for class 2 is small and yet the loss for class 3 is about 7 times higher than that of class 2.\n",
        "#This makes the network focus more on class 3 during training ensuring better results for this class against easy to classify classes. \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average Focal Loss for class1:  tf.Tensor(0.00039390582, shape=(), dtype=float32)\n",
            "Average Focal Loss for class2:  tf.Tensor(0.029150667, shape=(), dtype=float32)\n",
            "Average Focal Loss for class3:  tf.Tensor(0.22109523, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
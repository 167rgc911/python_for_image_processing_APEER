{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tutorial94_DL_terminology_Activation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM5H5FlU1Yefup9r/kY7VND",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bnsreenu/python_for_image_processing_APEER/blob/master/tutorial94_DL_terminology_Activation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCEnyiOPFTaB"
      },
      "source": [
        "https://youtu.be/vp8mQ_inplo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIySFtVUxH5f"
      },
      "source": [
        "**Code to explain Activation Functions**\n",
        "\n",
        "Example use case - A family deciding on the cuisine to eat out. \n",
        "Choices are, Thai, Sushi and Indian. \n",
        "\n",
        "Each family member has weight associated with their pwoer to influence the decision.\n",
        "\n",
        "Mom: 0.5\n",
        "\n",
        "Dad: 0.2\n",
        "\n",
        "Son: 0.3\n",
        "\n",
        "Daughter: 0.6\n",
        "\n",
        "\n",
        "Each member provides a rating for each cuisine type that ranges between -100 (really hate) to 100 (really love).\n",
        "\n",
        "Ratings for each cusine given by Mom, da, son, and daughter, respectively, in that order.\n",
        "\n",
        "Thai: -30, 100, 40, -40\n",
        "\n",
        "Sushi: -10, -25, -60, 50\n",
        "\n",
        "Indian: -50, -50, 50, 40 \n",
        "\n",
        "Weighted sum as calculated by the neuron for each cuisine type by adding the product of above weights and ratings by each family member:\n",
        "\n",
        "Thai: -7 \n",
        "\n",
        "Sushi: 2\n",
        "\n",
        "Indian: 4\n",
        "\n",
        "These will be used as inputs to each activation function velow. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "df-F_sCspyss"
      },
      "source": [
        "import numpy as np\n",
        "my_data = np.array([-7, 2, 4])  #Thai, Sushi, Indian, respectively. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sPOt0IJqh7C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a367039b-d299-44c5-f5c9-462d1acf4034"
      },
      "source": [
        "# Linear kernel. Input values with some scaling constant \n",
        "# Gradient (derivative) will be a constant. This means the update factor for weights and biases during training is the same. \n",
        "# The network will not be improving better during training, especially for complex scenarios. \n",
        "\n",
        "def linear(x):\n",
        "    return 2*x  #Scaling constant = 2\n",
        "\n",
        "linear(my_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-14,   4,   8])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cENKmqhtqVkL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0ef6ca8-4dea-43f4-9c50-30d2371d24a4"
      },
      "source": [
        "#Step function. 0 for all values below 0 and 1 for all values above 0.\n",
        "# Useful for binary classification and useless for multiclass classification. \n",
        "# Gradient of step function is zero, so do not use this for hidden layers as the weights will not be updated. \n",
        "# Use this for the classification layer. \n",
        "\n",
        "def step(x):\n",
        "    if x<0:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "    print(i)\n",
        "\n",
        "print(step(-7))\n",
        "print(step(2))\n",
        "print(step(4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XT3mfH3HrB7L",
        "outputId": "627ea7d6-5b6b-419e-bab2-fdfcabfe027e"
      },
      "source": [
        "#sigmoid function - transforms input values to values between 0 and 1\n",
        "# Continuously differentialble function. The derivative tapers off (gets small) pretty fast, around values -4 / 4.\n",
        "# This means the network is not learning when these limits are reached. \n",
        "# Output values do not all add up to 1. \n",
        "\n",
        "def sigmoid(X):\n",
        "   return 1/(1+np.exp(-X))\n",
        "\n",
        "sigm_result = sigmoid(my_data)\n",
        "\n",
        "print(sigm_result)\n",
        "\n",
        "sigm_result_binarized = sigm_result>0.5\n",
        "print(sigm_result_binarized)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9.11051194e-04 8.80797078e-01 9.82013790e-01]\n",
            "[False  True  True]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QZJQfjct91A",
        "outputId": "ac08f2f1-c7e4-45d3-a6f2-21c8f89bc8d5"
      },
      "source": [
        "#tanh(x)=2sigmoid(2x)-1\n",
        "# Similar to sogmoid except this is symmetric around origin and ranges from -1 to 1. \n",
        "# Usually tanh is preferred over the sigmoid function as it is zero centered\n",
        "\n",
        "def tanh(x):\n",
        "    return (2/(1 + np.exp(-2*x))) -1\n",
        "\n",
        "tanh_result=tanh(my_data)\n",
        "\n",
        "print(tanh_result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.99999834  0.96402758  0.9993293 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_l9CJq7toHY",
        "outputId": "f76dfa3e-1641-4e19-f4ac-a9a0f9c354bc"
      },
      "source": [
        "#ReLu - Rectified Linear Unit\n",
        "# Neurons will only be deactivated if the output of the transformation is less than 0.\n",
        "# Output is 0 for negative values and linear for values above 0. \n",
        "# negative side og the derivative (gradient) is 0, this may lead to dead neurons that never get activated.\n",
        "# Leaky ReLU can be used if this is a problem. \n",
        "def relu(X):\n",
        "   return np.maximum(0,X)\n",
        "\n",
        "relu(my_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 2, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8XwFPScum5H",
        "outputId": "a0a921cd-4723-4e54-97cd-bfa7becd38a6"
      },
      "source": [
        "# Leaky ReLu\n",
        "#Similar to reLU except for values below zero instead of output being 0 it is a tiny linear component. \n",
        "def leaky_relu(x):\n",
        "    if x<0:\n",
        "        return 0.1*x\n",
        "    else:\n",
        "        return x\n",
        "print(leaky_relu(-7))\n",
        "print(leaky_relu(2))\n",
        "print(leaky_relu(4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-0.7000000000000001\n",
            "2\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqT2Jzobtvsg",
        "outputId": "8f2ab1c8-c974-40fa-d10e-1ce9353a16c9"
      },
      "source": [
        "#Softmax is like combining multiple sigmoids. \n",
        "# Softmax is used for multiclass classification problems. \n",
        "# Output returns the probability for a datapoint belonging to each individual class.\n",
        "#All probabilities add to 1\n",
        "\n",
        "def softmax(X):\n",
        "    expo = np.exp(X)\n",
        "    expo_sum = np.sum(np.exp(X))\n",
        "    return expo/expo_sum\n",
        "\n",
        "#Example with mmatrix defined above\n",
        "a, b, c = softmax(my_data)\n",
        "print (softmax(my_data))\n",
        "print(\"The sum of all values = \", a+b+c)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.47105928e-05 1.19201168e-01 8.80784121e-01]\n",
            "The sum of all values =  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA6BCgH_3tpb"
      },
      "source": [
        "**Suggestions:**\n",
        "\n",
        "\n",
        "*   Sigmoid works well for classifiers\n",
        "*   Sigmoid (and tanh) are usually avoided in hidden layers due to the vanishing gradient problem\n",
        "*   ReLU is often used in hidden layers, especially for CNNs.\n",
        "*   Never use ReLU outside the hidden layers.\n",
        "*   Use softmax for multiclass classification\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}